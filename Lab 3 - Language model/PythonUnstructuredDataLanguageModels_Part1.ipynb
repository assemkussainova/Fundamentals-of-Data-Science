{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRj3mE-woslO"
   },
   "source": [
    "# Part 1\n",
    "In the first part, we will learn how to work with unstructured data.\n",
    "In particular, we will work with text data to a build language model (LM). \n",
    "\n",
    "LM is a probability distribution $p(\\cdot)$ over some text $T$.\n",
    "For example, if $T=(w_1\\ w_2\\ \\dots \\ w_{|T|})$, where $w_1,\\ w_2\\, \\dots \\ w_{|T|}$ is  collection of words and $|T|$ is the number of words in the text, LM is computed as\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots \\ w_{|T|})$\n",
    "\n",
    "LM is employed in many applications such as automatic speech recognition (ASR), machine translation (MT), image captioning, text generation and etc. For example, LM is used in ASR to assess correctness of generated output transcripts.\n",
    "\n",
    "Let's assume our ASR system generated two possible transcripts for some given speech signal: \"recognize speech\" and \"wreck a nice beach\".\n",
    "These two phrases sound similar, but the first one is more likely to be linguistically correct.\n",
    "We can use the probability of each transcript to determine the final output of ASR.\n",
    "The probabilistic model (LM) should assign a higher probability score to the correct answer:\n",
    "\n",
    "$p(``\\text{recognize speech}\") > p(``\\text{wreck a nice beach}\")$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eHXr0-sP1dl"
   },
   "source": [
    "### Reading a text file\n",
    "We will use Penn Tree Bank ([PTB](https://catalog.ldc.upenn.edu/LDC99T42)) dataset as our text data in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDXB-a8nOU0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'str'>\n",
      "Size: 5101618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a file into the string\n",
    "with open('ptb.train.txt') as f:\n",
    "   raw_data = f.read()\n",
    "\n",
    "print(\"Type: \" + str(type(raw_data)))  # type of the data\n",
    "print(\"Size: \" + str(len(raw_data)))   # number of characters\n",
    "raw_data[967:1156]                     # print a small snippet of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF0q7uNEX-iP"
   },
   "source": [
    "### Text preprocessing\n",
    "Take a look at the PTB dataset. It is already pre-processed, i.e. words are tokenized, all letters are lowercased and etc. We need to apply a few small changes for our assignment. Please note, only the most frequient 10000 words are kept in the dataset, the rest of the words are replaced with <unk> symbol.This is done to reduce memory and computation requirements.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hy6VmkadXFb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Size: 42068\n",
      "Sentence:  mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n"
     ]
    }
   ],
   "source": [
    "# Split the raw data into sentences and save them into a list (delimiter for sentence boundaries: '\\n')\n",
    "data = raw_data.split('\\n')\n",
    "data = [sent for sent in data if sent != ''] # remove empty sentences\n",
    "\n",
    "print(\"Type: \" + str(type(data)))\n",
    "print(\"Size: \" + str(len(data)))     # number of sentences\n",
    "print(\"Sentence: \" + data[2])        # print a sentence, try to print another sentence (symbol '<unk>' corresponds to unknown word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwXYBvmRH0Rr"
   },
   "source": [
    "### Your turn - more preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGDUm8FwH0Rs"
   },
   "outputs": [],
   "source": [
    "# We need to add '<bos>' and '<eos>' symbols to the beginning and ending of each sentence respectively, e.g. \"how are you\"  => \"<bos> how are you <eos>\"\n",
    "# To do so, first copy the data to data2 so that we do not modify data. Hint: use a function to perform a shallow copy\n",
    "# Then make the changes to data2\n",
    "data2 = data.copy()\n",
    "for i in range(len(data2)):\n",
    "    data2[i] = ''.join(('<bos>', data2[i],'<eos>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ou_4caErH0Ru"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      "Sentence: <bos> aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos>\n"
     ]
    }
   ],
   "source": [
    "# If you completed the task correctly, the first printed sentence will not have the inserted symbols\n",
    "print(\"Sentence: \" + data[0])\n",
    "print(\"Sentence: \" + data2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_U0G40kJYR4i"
   },
   "source": [
    "### Your turn - word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ugtyrCeH0R0"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary which contains a word as a key and its frequency as a value, eg. 'judge': 262\n",
    "# Hint: use split() function again to divide each sentence into a list of words\n",
    "dictionary = dict()\n",
    "for string in data2:\n",
    "    s = string.split()\n",
    "    for w in s:\n",
    "        if w in dictionary:\n",
    "            dictionary[w] += 1\n",
    "        else:\n",
    "            dictionary[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AhrqU_fXLO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 10001\n",
      "262\n"
     ]
    }
   ],
   "source": [
    "# If you completed the task correctly, the number of unique words shold be 10001 and the frequency of 'judge' should be 262\n",
    "print(\"Size: \"+str(len(dictionary))) # number of unique words\n",
    "print(dictionary['judge']) # try other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K14WGWlfPMBc"
   },
   "source": [
    "### Building LM (probabilistic model)\n",
    "To build a probabilistic model of a text, we can employ the frequentist approach, i.e. use relative frequency to estimate probability scores.\n",
    "Suppose we want to estimate probability of a sentence \"how are you\", then we need to count how many times it appears in the dataset. This approach is infeasible for long and/or complex sentences that might be absent or appear a few times in the dataset.\n",
    "\n",
    "Computing probability of short phrase is easy (click [here](https://books.google.com/ngrams/graph?content=How+are+you&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2CHow%20are%20you%3B%2Cc0#t1%3B%2CHow%20are%20you%3B%2Cc0)).\n",
    "However, for a longer sentence it is difficult (click [here](https://books.google.com/ngrams/graph?content=I+met+my+friend+Madina&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=))\n",
    "\n",
    "#### Review: Chain rule\n",
    "To circumvent the problem mentioned above, we will employ the chain rule to break down the joint probability of all words in a sentence into the sequence of conditional probabilities as follows:\n",
    "\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots\\ w_{|T|}) = p(w_1)\\ p(w_2 | w_1)\\ p(w_3 | w_1\\ w_2)\\ \\dots \\ p(w_{|T|} | w_1 \\dots w_{|T|-1}) = p(w_1)\\prod^{|T|}_{i=2}p(w_i|w_1\\ \\dots\\ w_{i-1})$\n",
    "\n",
    "where $p(w_3 | w_1\\ w_2)$ - is the probability of word $w_3$ given that the preceeding two words in the sentence were $w_1$ and $w_2$.\n",
    "\n",
    "For example, if $T= \\text{\"How are you\"}$, then $w_1=\"How\"$, $w_2 = \"are\"$ and $w_3 = \"you\"$, then the probability of having such sentence is $p(\\text{\"How are you\"})= p(\"How\")\\ p(\"are\"| \"How\")\\ p(\"you\" | \"How\" \\ \"are\")$\n",
    "\n",
    "\n",
    "The conditional probabilities are estimated as follows:\n",
    "\n",
    "$p(w_i|w_{i-1})=\\frac{count(w_{i-1}\\ w_{i})}{count(w_{i-1})}$\n",
    "\n",
    "\n",
    "Nevertheless, this is still infeasible for long sentences, because computing conditional probability of the last words still requires to count occurences of the preceding long phrases.\n",
    "\n",
    "The simplest way to solve this problem is to treat each word independent from each other, i.e. $p(w_2|w_1)=p(w_2)$, $p(w_3| w_1\\ w_2)=p(w_3)$ and so on (link to probability independence):\n",
    "\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots\\ w_{|T|}) \\approx p(w_1)\\ p(w_2)\\ p(w_3)\\ \\dots \\ p(w_{|T|}) = \\prod^{|T|}_{i=1}p(w_i)$\n",
    "\n",
    "\n",
    "The marginal probability of each word can be estimated as follows:\n",
    "\n",
    "$p(w_i)=\\frac{count(w_i)}{\\sum count(w)} = \\frac{count(w_i)}{Total\\ number\\ of\\ words}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1hyePfXQUzQ"
   },
   "source": [
    "### Your turn - estimating probability of a word\n",
    "Create a dictionary called **dictionary_prob** which holds the marginal probability of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BzNdZRPQPD5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002696424767176071"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sum(dictionary.values())\n",
    "\n",
    "dictionary_prob = dict()\n",
    "\n",
    "for i in dictionary:\n",
    "    dictionary_prob[i] = dictionary[i]/s \n",
    "\n",
    "dictionary_prob['judge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eatPD24cS_lC"
   },
   "source": [
    "### Your turn - estimating probability of a sentence\n",
    "Write a function called **compute_prob0** which takes a sentence and dictionary of marginal probabilites as input and returns its probability assuming each word is independent. For the words not present in the dictionary, probability of '\\<unk>' symbol must be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAbDeDy6TIbh"
   },
   "outputs": [],
   "source": [
    "def compute_prob0(sentence, dictionary_prob):\n",
    "    string = sentence.split()\n",
    "    prob = 1\n",
    "    for w in string:\n",
    "        if w in dictionary_prob:\n",
    "            prob *= dictionary_prob[w]\n",
    "        else:\n",
    "            prob *= dictionary_prob['<unk>']\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iR9RKJIKTyEk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.231238353466466e-11"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you completed the task correctly, it should return 6.231238353466466e-11\n",
    "sentence = \"hello how are you\"\n",
    "compute_prob0(sentence, dictionary_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PythonUnstructuredDataLanguageModels_Part1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
